{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Who was Jim Henson ? Jim Henson was a puppeteer\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "indexed_tokens = [101] + tokenizer.convert_tokens_to_ids(tokenized_text) + [102]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who',\n",
       " 'was',\n",
       " 'jim',\n",
       " 'henson',\n",
       " '?',\n",
       " 'jim',\n",
       " 'henson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data\n",
    "f = open(\"wiki-0.1percent.txt\", \"r\")\n",
    "wiki = f.readlines()\n",
    "data = []\n",
    "for idx, line in enumerate(wiki):\n",
    "    result = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line.strip())[:510])\n",
    "    result = [101] + result + [102] + [0]*(510 - len(result))\n",
    "    data.append(result)\n",
    "t_data = torch.LongTensor(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_constrative(data, N):\n",
    "    idx_list = np.random.choice(np.arange(0, data.shape[0]),N, replace= False)\n",
    "    return data[idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 16\n",
    "collect_num = 3\n",
    "emb_size = 768\n",
    "sample1 = sampling_constrative(t_data, batch_size*collect_num)\n",
    "sample2 = sampling_constrative(t_data, batch_size*collect_num)\n",
    "_, sentence_emb1 = model(torch.LongTensor(sample1))\n",
    "_, sentence_emb2 = model(torch.LongTensor(sample2))\n",
    "#sentence_emb = sentence_emb.view(batch_size, collect_num, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = torch.nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(e, squared=False, eps=1e-12):\n",
    "    e_square = e.pow(2).sum(dim=1)\n",
    "    prod = e @ e.t()\n",
    "    res = (e_square.unsqueeze(1) + e_square.unsqueeze(0) - 2 * prod).clamp(min=eps)\n",
    "\n",
    "    if not squared:\n",
    "        res = res.sqrt()\n",
    "\n",
    "    res = res.clone()\n",
    "    res[range(len(e)), range(len(e))] = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RkdDistance(nn.Module):\n",
    "    def forward(self, student, teacher):\n",
    "        with torch.no_grad():\n",
    "            t_d = pdist(teacher, squared=False)\n",
    "            mean_td = t_d[t_d>0].mean()\n",
    "            t_d = t_d / mean_td\n",
    "\n",
    "        d = pdist(student, squared=False)\n",
    "        mean_d = d[d>0].mean()\n",
    "        d = d / mean_d\n",
    "\n",
    "        loss = F.smooth_l1_loss(d, t_d, reduction='mean')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDistance = RkdDistance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2391, grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDistance(sentence_emb1, sentence_emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dim : [batch_size, num_tuple ,embedding_size] # [64, 2, 768]\n",
    "def distance_wise_relation_function(input):\n",
    "    assert input.shape[1] == 2, \"num_tuple must be 2.\"\n",
    "    \n",
    "    size = input.shape[0]\n",
    "    batch_sum = 0\n",
    "    vector = torch.empty(size = (size,1))\n",
    "    for i in range(size):\n",
    "        x_i = input[i][0]\n",
    "        x_j = input[i][1]\n",
    "        sub = torch.sub(x_i, x_j).norm(p=2)\n",
    "        vector[i] = sub\n",
    "        batch_sum += sub\n",
    "    return vector*size*(1/batch_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "num_tuple must be 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79839/4236462543.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_wise_relation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_79839/3040021228.py\u001b[0m in \u001b[0;36mdistance_wise_relation_function\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# input dim : [batch_size, num_tuple ,embedding_size] # [64, 2, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistance_wise_relation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_tuple must be 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: num_tuple must be 2."
     ]
    }
   ],
   "source": [
    "relation = distance_wise_relation_function(sentence_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_loss = RKdAngle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinjk1156/.conda/envs/jk_lab/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2838, grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_loss(sentence_emb1, sentence_emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# bert 에 dropout이 있으니까 그냥 encoder에 두번 똑같은 sentence를 넣으면 된데. train mode에서 그런듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.Tensor(np.random.normal(1,1, size = (1,3,7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = features.shape[0]\n",
    "mask = torch.eye(batch_size, dtype=torch.float32)\n",
    "contrast_count = features.shape[1]\n",
    "contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "#anchor_feature = contrast_feature\n",
    "anchor_feature = features[:, 0]   # 전체 다 가져옴\n",
    "#anchor_count = contrast_count\n",
    "anchor_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_dot_contrast = torch.div(torch.matmul(anchor_feature, contrast_feature.T), 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.1038, 6.4583, 6.5186]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits = anchor_dot_contrast - logit_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.6455, -0.5853]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.repeat(anchor_count, contrast_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(batch_size * (anchor_count)).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_mask = torch.scatter(torch.ones_like(mask), 1, torch.arange(batch_size * anchor_count).view(-1, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask * logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_logits = torch.exp(logits) * logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5244, 0.5570]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0782]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(exp_logits.sum(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0814]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_logits.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.6455, -0.5853]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6936])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_log_prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -1 * mean_log_prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss.view(anchor_count, batch_size).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.LongTensor(list(map(int, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/shinjk1156/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788c67e97627497ab5d2b5a41d2ae3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataload import get_data, load_data, construct_data_for_finetuning\n",
    "t_data1, t_data2, labels = get_data(data_type=\"test\", task = \"stsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration sick\n",
      "Reusing dataset sick (/home/shinjk1156/.cache/huggingface/datasets/sick/sick/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baebf6311d274e30a2929163919c0028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_data1, t_data2, labels = get_data(data_type=\"test\", task = \"sick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, token_ids = construct_data_for_finetuning(t_data1, t_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "idices =(t_data1 == 102).nonzero(as_tuple=True)[1]\n",
    "data1 = t_data1.clone()\n",
    "for i in range(idices.shape[0]):\n",
    "    data1[i][idices[i] + 1:] = t_data2[i,1:512- (idices[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration sick\n",
      "Reusing dataset sick (/home/shinjk1156/.cache/huggingface/datasets/sick/sick/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2db68f83b464789b6c26df78fa0ee1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_data1, t_data2, labels = get_data(data_type=\"test\", task = \"sick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete! Data loading\n"
     ]
    }
   ],
   "source": [
    "t_loader = load_data(t_data1, 'cpu', 16, shuffle_true=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, attn in t_loader:\n",
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-42c6c3d51c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36d7992fc81c8f34731f5e7dce2e5b34394249160d51abd8a418d59f2acc7620"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('jk_lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
